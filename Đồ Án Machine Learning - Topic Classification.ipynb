{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Đồ Án Machine Learning - Topic Classification.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMPxJKL+MtIz9zU5ulByVKp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"j6NHwQhZkI6v"},"source":["Dữ liệu\r\n","https://drive.google.com/drive/folders/1vA9yta8dDBP8eJOfwh8JItJuqwSCyMfS?usp=sharing"]},{"cell_type":"markdown","metadata":{"id":"O1_Drw8WB_Y_"},"source":["#Xử lý dữ liệu"]},{"cell_type":"code","metadata":{"id":"ne-eRrXw69S4"},"source":["!unzip \"/content/drive/My Drive/D/Đồ Án.zip\" -d \"./Data\"\r\n","%cp \"/content/Data/Đồ Án/NLP 02/Stats.txt\" -d '/content/Data'\r\n","!pip install unrar\r\n","!unrar x \"/content/Data/Đồ Án/NLP 02/Test.rar\" -d \"./Data\"\r\n","!unrar x \"/content/Data/Đồ Án/NLP 02/Train.rar\" -d \"./Data\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wsy_SLUB0cC"},"source":["!git clone https://github.com/stopwords/vietnamese-stopwords.git\r\n","%cp \"/content/vietnamese-stopwords/vietnamese-stopwords-dash.txt\" -d \"/content/Data\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPIS8JTiB2tX"},"source":["def PhanLoai(Topic):\r\n","  TopicIDs = {\"Am nhac\":\"AN\", \"Am thuc\":\"AT\", \"Bat dong san\":\"BDS\", \"Bong da\":\"BD\", \"Chung khoan\":\"CK\",\r\n","              \"Cum ga\":\"CG\", \"Cuoc song do day\":\"CSDD\", \"Du hoc\":\"DH\", \"Du lich\":\"DL\", \"Duong vao WTO\":\"DVW\",\r\n","              \"Gia dinh\":\"GD\", \"Giai tri tin hoc\":\"GTTH\", \"Giao duc\":\"GDu\", \"Gioi tinh\":\"GT\", \"Hackers va Virus\":\"HV\",\r\n","              \"Hinh su\":\"HS\", \"Khong gian song\":\"KGS\", \"Kinh doanh quoc te\":\"KDQT\", \"Lam dep\":\"LD\", \"Loi song\":\"LS\",\r\n","              \"Mua sam\":\"MS\", \"My thuat\":\"MT\", \"San khau dien anh\":\"SKDA\", \"San pham tin hoc moi\":\"SPTHM\", \"Tennis\":\"T\",\r\n","              \"The gioi tre\":\"TGT\", \"Thoi trang\":\"TT\"}\r\n","  return TopicIDs[Topic]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NqFFCKRcB5aT"},"source":["# Hàm tiền xử lý dữ liệu\r\n","def Preprocessing(path):\r\n","  files = []\r\n","  ids = []\r\n","\r\n","  stopword = []\r\n","  with open(\"/content/Data/vietnamese-stopwords-dash.txt\", \"r\") as f:\r\n","    stopword = f.read().splitlines()\r\n","\r\n","# Lấy các file .txt\r\n","  # r=root, d=directories, f = files\r\n","  for r, d, f in os.walk(path):\r\n","      for folder in d:\r\n","          id = PhanLoai(folder)\r\n","          for r1, d1, f in os.walk(os.path.join(r, folder)):\r\n","            for file in f:\r\n","              files.append(os.path.join(r1, file))\r\n","              ids.append(id)\r\n","\r\n","  X = []\r\n","  for file in tqdm(files, desc='Processing', position=0):\r\n","    with open(file, 'r', encoding='utf-16') as f:\r\n","      # Ghép các từ riêng lẻ thành cụm từ có nghĩa\r\n","      txt = ViTokenizer.tokenize(f.read())\r\n","      # Xóa các ký tự đặc biệt không phải chữ hoặc số\r\n","      txt = strip_non_alphanum(txt).lower().strip()\r\n","      # Xóa ký tự là số\r\n","      txt = strip_numeric(txt)\r\n","      # Xóa từ có số và chữ, vd a1 a2\r\n","      txt = split_alphanum(txt)\r\n","      # Xóa các chữ đứng riêng lẻ\r\n","      txt = strip_short(txt, minsize=2)\r\n","      # Chuẩn hóa để mỗi từ cách nhau một khoảng trắng\r\n","      txt = strip_multiple_whitespaces(txt)\r\n","      # Tách cầu thành từ riêng lẻ và xóa stopword\r\n","      txt = txt.split()\r\n","      txt = [word for word in txt if word not in stopword]\r\n","      # Hợp lại thành câu\r\n","      txt = \" \".join(txt)\r\n","      X.append(txt)\r\n","\r\n","  return X, ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLVsFuDFB7U2"},"source":["# Xử lý dữ liệu\r\n","path = \"/content/Data/new train\"\r\n","X_train, Y_train = Preprocessing(path)\r\n","path = \"/content/Data/new test\"\r\n","X_test, Y_test = Preprocessing(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGZoiSSqB89u"},"source":["# Lưu lại dữ liệu đã tiền xử lý\r\n","pickle.dump(X_train, open('Data/X_train.pkl', 'wb'))\r\n","pickle.dump(Y_train, open('Data/Y_train.pkl', 'wb'))\r\n","pickle.dump(X_test, open('Data/X_test.pkl', 'wb'))\r\n","pickle.dump(Y_test, open('Data/Y_test.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cxku0TB3Ew0u"},"source":["#Load data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_MBHeWo1EP9u","executionInfo":{"status":"ok","timestamp":1607879203283,"user_tz":-420,"elapsed":21463,"user":{"displayName":"Long Nguyen Duc Huy","photoUrl":"","userId":"14217015565815796984"}},"outputId":"172e7085-196b-4a22-cdc1-c7fd253532d1"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1UPZi5noEaFh","executionInfo":{"status":"ok","timestamp":1607879785930,"user_tz":-420,"elapsed":3931,"user":{"displayName":"Long Nguyen Duc Huy","photoUrl":"","userId":"14217015565815796984"}}},"source":["%cp \"/content/drive/MyDrive/Data_Do_An_ML/X_train.pkl\" -d \"/content/Data\"\r\n","%cp \"/content/drive/MyDrive/Data_Do_An_ML/X_test.pkl\" -d \"/content/Data\"\r\n","%cp \"/content/drive/MyDrive/Data_Do_An_ML/Y_train.pkl\" -d \"/content/Data\"\r\n","%cp \"/content/drive/MyDrive/Data_Do_An_ML/Y_test.pkl\" -d \"/content/Data\"\r\n","%cp \"/content/drive/MyDrive/Data_Do_An_ML/X_train_trans.pkl\" -d \"/content/Data\"\r\n","%cp \"/content/drive/MyDrive/Data_Do_An_ML/X_test_trans.pkl\" -d \"/content/Data\"\r\n","%cp \"/content/drive/MyDrive/Data_Do_An_ML/X_train_tf.pkl\" -d \"/content/Data\"\r\n","%cp \"/content/drive/MyDrive//Data_Do_An_ML/X_test_tf.pkl\" -d \"/content/Data\""],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TDiBXXr6jHJ8"},"source":["#K Nearest Neighbors"]},{"cell_type":"code","metadata":{"id":"HONfxXK7i8mc","executionInfo":{"status":"ok","timestamp":1607881077883,"user_tz":-420,"elapsed":821,"user":{"displayName":"Long Nguyen Duc Huy","photoUrl":"","userId":"14217015565815796984"}}},"source":["import time\r\n","import pickle\r\n","import numpy as np\r\n","import pandas as pd\r\n","from tqdm import tqdm\r\n","import matplotlib.pyplot as plt\r\n","from sklearn.pipeline import Pipeline\r\n","from sklearn.neighbors import KNeighborsClassifier\r\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, plot_confusion_matrix"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDSeVK4l7Nn6","executionInfo":{"status":"ok","timestamp":1607880998269,"user_tz":-420,"elapsed":1061,"user":{"displayName":"Long Nguyen Duc Huy","photoUrl":"","userId":"14217015565815796984"}}},"source":["X_train = pickle.load(open('/content/Data/X_train_trans.pkl', 'rb'))\r\n","X_test = pickle.load(open('/content/Data/X_test_trans.pkl', 'rb'))\r\n","Y_train = pickle.load(open('/content/Data/Y_train.pkl', 'rb'))\r\n","Y_test =  pickle.load(open('/content/Data/Y_test.pkl', 'rb'))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"SYcwhVg47Uw9"},"source":["start = time.time()\r\n","a = [1, 3, 5, 7, 9, 11, 13, 15 ,17 ,19]\r\n","b = [\"euclidean\", \"cosine\", \"minkowski\"]\r\n","c = [\"uniform\", \"distance\"]\r\n","df = pd.DataFrame(columns=('time', 'n_neibors', 'metric', 'weights', 'f1_score', 'accuracy_score', 'precision_score', 'recall_score'))\r\n","for i in tqdm(a, position=0, leave=True):\r\n","  for j in b:\r\n","    for l in c:\r\n","      t = time.time()\r\n","      model = KNeighborsClassifier(n_neighbors= i, metric= j, weights= l)\r\n","      model.fit(X_train, Y_train)\r\n","      Y_pred = model.predict(X_test)\r\n","      for k in [\"micro\", \"macro\", \"weighted\"]:\r\n","        d1 = f1_score(Y_test, Y_pred, average=k)\r\n","        d2 = accuracy_score(Y_test, Y_pred)\r\n","        d3 = precision_score(Y_test, Y_pred, average = k)\r\n","        d4 = recall_score(Y_test, Y_pred, average = k)\r\n","        row = pd.Series({'time':time.time()-t, 'n_neibors':i, 'metric':j, 'average':k, 'weights':l, 'f1_score':d1, 'accuracy_score':d2, 'precision_score':d3, 'recall_score':d4})\r\n","        df = df.append(row, ignore_index=True)\r\n","print(time.time() - start)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_O_8jxNI7Xre"},"source":["start = time.time()\r\n","a = [1, 3, 5, 7, 9, 11, 13, 15 ,17 ,19]\r\n","b = [\"euclidean\", \"cosine\", \"minkowski\"]\r\n","c = [\"uniform\", \"distance\"]\r\n","c1=c2= c3= c4 = 0\r\n","df = pd.DataFrame(columns=('time', 'n_neibors', 'metric', 'weights', 'f1_score', 'accuracy_score', 'precision_score', 'recall_score'))\r\n","for i in a:\r\n","  for j in b:\r\n","    for l in c:\r\n","      t = time.time()\r\n","      model = KNeighborsClassifier(n_neighbors= i, metric= j, weights= l)\r\n","      model.fit(X_train, Y_train)\r\n","      Y_pred = model.predict(X_test)\r\n","      for k in [\"micro\", \"macro\", \"weighted\"]:\r\n","        d1 = f1_score(Y_test, Y_pred, average=k)\r\n","        d2 = accuracy_score(Y_test, Y_pred)\r\n","        d3 = precision_score(Y_test, Y_pred, average = k)\r\n","        d4 = recall_score(Y_test, Y_pred, average = k)\r\n","        if (d1>c1):\r\n","          m1 = model\r\n","          c1 = d1\r\n","        if (d2>c2):\r\n","          m2 = model\r\n","          c2 = d2\r\n","        if (d3>c3):\r\n","          m3 = model\r\n","          c3 = d3\r\n","        if (d4>c4):\r\n","          m4 = model\r\n","          c3 = d4\r\n","        row = pd.Series({'time':time.time()-t, 'n_neibors':i, 'metric':j, 'average':k, 'weights':l, 'f1_score':d1, 'accuracy_score':d2, 'precision_score':d3, 'recall_score':d4})\r\n","        df = df.append(row, ignore_index=True)\r\n","        print(\"done\")\r\n","print(time.time() - start)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IwLh4Y9J7aQF"},"source":["print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hiiRskL-7dc-"},"source":["df.to_csv('/content/Data/KNN/export.csv', index=False)\r\n","pickle.dump(m1, open('/content/Data/KNN/model_KNN_max_f1.pkl', 'wb'))\r\n","pickle.dump(m2, open('/content/Data/KNN/model_KNN_max_acc.pkl', 'wb'))\r\n","pickle.dump(m3, open('/content/Data/KNN/model_KNN_max_pre.pkl', 'wb'))\r\n","pickle.dump(m4, open('/content/Data/KNN/model_KNN_max_rec.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z5Kf4_B77e1d"},"source":["title = \"CM\"\r\n","fig, ax = plt.subplots(figsize=(14,14))\r\n","disp = plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","disp.ax_.set_title(title)\r\n","print(title)\r\n","print(disp.confusion_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cpmt0gkbjObe"},"source":["#Logistic Regression"]},{"cell_type":"code","metadata":{"id":"24vvN3GtjVnj","executionInfo":{"status":"ok","timestamp":1607882899237,"user_tz":-420,"elapsed":1227,"user":{"displayName":"Long Nguyen Duc Huy","photoUrl":"","userId":"14217015565815796984"}}},"source":["import pickle\r\n","import pandas\r\n","from tqdm import tqdm\r\n","import matplotlib.pyplot as plt\r\n","from sklearn.metrics import plot_confusion_matrix\r\n","from sklearn.linear_model import LogisticRegression\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"dveUWAWI-tDt","executionInfo":{"status":"ok","timestamp":1607882899788,"user_tz":-420,"elapsed":1424,"user":{"displayName":"Long Nguyen Duc Huy","photoUrl":"","userId":"14217015565815796984"}}},"source":["X_train = pickle.load(open('/content/Data/X_train.pkl', 'rb'))\r\n","X_test = pickle.load(open('/content/Data/X_test.pkl', 'rb'))\r\n","Y_test = pickle.load(open('/content/Data/Y_test.pkl', 'rb'))\r\n","Y_train = pickle.load(open('/content/Data/Y_train.pkl', 'rb'))\r\n","X_train_trans = pickle.load(open('/content/Data/X_train_tf.pkl', 'rb'))\r\n","X_test_trans = pickle.load(open('/content/Data/X_test_tf.pkl', 'rb'))"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"G5xLaqYI-0V8"},"source":["b = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\r\n","for k in tqdm(b, position=0, leave=True):\r\n","  model = LogisticRegression(solver=k)\r\n","  model.fit(X_train_trans, Y_train)\r\n","  f1 = f1_score(Y_test, model.predict(X_test_trans), average=\"micro\")\r\n","  print(k, f1)\r\n","  #title = \"CM\"\r\n","  #fig, ax = plt.subplots(figsize=(14,14))\r\n","  #plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","  #plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CL_kxXkk-2p7"},"source":["b = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\r\n","for k in tqdm(b, position=0, leave=True):\r\n","  model = LogisticRegression(solver=k)\r\n","  model.fit(X_train_trans, Y_train)\r\n","  precision = precision_score(Y_test, model.predict(X_test_trans), average=\"micro\")\r\n","  print(k, precision)\r\n","  #title = \"CM\"\r\n","  #fig, ax = plt.subplots(figsize=(14,14))\r\n","  #plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","  #plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WZukmMA-64b"},"source":["b = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\r\n","for k in tqdm(b, position=0, leave=True):\r\n","  model = LogisticRegression(solver=k)\r\n","  model.fit(X_train_trans, Y_train)\r\n","  recall = recall_score(Y_test, model.predict(X_test_trans), average=\"micro\")\r\n","  print(k, recall)\r\n","  #title = \"CM\"\r\n","  #fig, ax = plt.subplots(figsize=(14,14))\r\n","  #plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","  #plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WU_874-F-819"},"source":["b = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\r\n","for k in tqdm(b, position=0, leave=True):\r\n","  model = LogisticRegression(solver=k)\r\n","  model.fit(X_train_trans, Y_train)\r\n","  accuracy = accuracy_score(Y_test, model.predict(X_test_trans), normalize=\"True\")\r\n","  print(k, accuracy)\r\n","  #title = \"CM\"\r\n","  #fig, ax = plt.subplots(figsize=(14,14))\r\n","  #plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","  #plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bx5dUu2njZ2n"},"source":["#Support Vector Machine"]},{"cell_type":"code","metadata":{"id":"3yk95P6CAmAn","executionInfo":{"status":"ok","timestamp":1607883382507,"user_tz":-420,"elapsed":276494,"user":{"displayName":"Long Nguyen Duc Huy","photoUrl":"","userId":"14217015565815796984"}}},"source":["# import những thư viện cần thiết\r\n","import pickle\r\n","import numpy as np\r\n","import pandas as pd\r\n","from tqdm import tqdm\r\n","from sklearn.svm import SVC\r\n","import matplotlib.pyplot as plt\r\n","from sklearn.pipeline import Pipeline\r\n","from sklearn.metrics import plot_confusion_matrix\r\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\r\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRydxPHTHlCN","executionInfo":{"status":"ok","timestamp":1607883382508,"user_tz":-420,"elapsed":276487,"user":{"displayName":"Long Nguyen Duc Huy","photoUrl":"","userId":"14217015565815796984"}}},"source":["# đọc dữ liệu đã biến đổi sang dạng vector và các dữ liệu liên quan\r\n","X_test = pickle.load(open('/content/Data/X_test.pkl', 'rb'))\r\n","X_train = pickle.load(open('/content/Data/X_train.pkl', 'rb'))\r\n","Y_train = pickle.load(open('/content/Data/Y_train.pkl', 'rb'))\r\n","Y_test = pickle.load(open('/content/Data/Y_test.pkl', 'rb'))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"n3jyaYNeAoPb"},"source":["for kernel in tqdm(['sigmoid', 'poly', 'rbf','linear']):\r\n","  SVM = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),max_df=0.8, max_features=None)),('tfidf', TfidfTransformer()),('clf', SVC(kernel=kernel, verbose=True,gamma='scale'))])\r\n","  SVM.fit(X_train,Y_train)\r\n","  # Lưu lại model\r\n","  pickle.dump(SVM, open('/content/Data/SVM/modelSVM_{}_Pipeline.pkl'.format(kernel), 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"49NREnJaBWRi"},"source":["SVM= ['sigmoid', 'poly', 'rbf','linear']\r\n","df=pd.DataFrame(columns=(' ','','Precision', 'F1 score', 'Recall', 'Accuracy', 'Accuracy_false'))\r\n","for svm in SVM :\r\n","  print('SVM: ',svm)\r\n","  model = pickle.load(open('/content/Data/SVM/modelSVM_{}_Pipeline.pkl'.format(svm), 'rb'))\r\n","  Y_pred = model.predict(X_test)\r\n","  row = pd.Series({' ':svm})\r\n","  df = df.append(row, ignore_index = True )\r\n","  for a in ('macro','micro','weighted'):\r\n","    print(\"Tham số truyền vào average: \", a)\r\n","    p = precision_score(Y_test, Y_pred,average= a)\r\n","    f = f1_score(Y_test, Y_pred, average= a)\r\n","    r = recall_score(Y_test, Y_pred, average= a)\r\n","    row = pd.Series({'':a,'Precision':p, 'F1 score':f, 'Recall':r})\r\n","    df = df.append(row, ignore_index = True )\r\n","    print(\"Precision: \",p)\r\n","    print(\"F1 score: \", f)\r\n","    print(\"Recall: \", r)\r\n","  cm = confusion_matrix(Y_test, model.predict(X_test))\r\n","  fig, ax = plt.subplots(figsize=(14,14))\r\n","  plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","  plt.show()\r\n","  # Tạo một dataframe dùng để lưu lại kết quả confusion_matrix\r\n","  da = pd.DataFrame(cm)\r\n","  # lưu lại kết quả confusion_matrix tập train\r\n","  da.to_csv(\"/content/Data/SVM/SVM_Pipeline_{}_test_cm.csv\".format(svm))\r\n","  ac = accuracy_score(Y_test,Y_pred, normalize=True)\r\n","  acf = accuracy_score(Y_test,Y_pred, normalize=False)\r\n","  row = pd.Series({'Accuracy':ac, 'Accuracy_false':acf})\r\n","  df = df.append(row, ignore_index = True )\r\n","  print(\"Accuracy: \", ac)\r\n","  print(\"Accuracy_false: \", acf)\r\n","df.to_csv('/content/Data/SVM/SVM_Kq_Pipeline.csv', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Em3UmvL_jWHm"},"source":["#Naive Bayes"]},{"cell_type":"code","metadata":{"id":"5mb5h7T8jZe2"},"source":["# import những thư viện cần thiết\r\n","import pickle\r\n","import numpy as np\r\n","import pandas as pd\r\n","from tqdm import tqdm\r\n","import matplotlib.pyplot as plt\r\n","from sklearn.pipeline import Pipeline\r\n","from sklearn.metrics import plot_confusion_matrix\r\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\r\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB\r\n","from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-e0VFEs__Qo"},"source":["# đọc dữ liệu đã biến đổi sang dạng vector và các dữ liệu liên quan\r\n","X_test = pickle.load(open('/content/Data/X_test.pkl', 'rb'))\r\n","X_train = pickle.load(open('/content/Data/X_train.pkl', 'rb'))\r\n","Y_train = pickle.load(open('/content/Data/Y_train.pkl', 'rb'))\r\n","Y_test = pickle.load(open('/content/Data/Y_test.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FPdakWWWAJfm"},"source":["# dùng pipeline để training mô hình NB GaussianNB\r\n","NB = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),max_df=0.8, max_features=None)),('tfidf', TfidfTransformer()),('clf',MultinomialNB())])\r\n","NB = NB.fit(X_train, Y_train)\r\n","pickle.dump(NB, open('/content/Data/NB/MultiNB_Pipeline.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5OxUbFOAKyH"},"source":["# dùng pipeline để training mô hình NB ComplementNB\r\n","NB = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),max_df=0.8, max_features=None)),('tfidf', TfidfTransformer()),('clf',ComplementNB())])\r\n","NB = NB.fit(X_train, Y_train)\r\n","pickle.dump(NB, open('/content/Data/NB/ComplementNB_Pipeline.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wGDXAPNFATRo"},"source":["# dùng pipeline để training mô hình NB BernoulliNB\r\n","NB = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),max_df=0.8, max_features=None)),('tfidf', TfidfTransformer()),('clf',BernoulliNB())])\r\n","NB = NB.fit(X_train, Y_train)\r\n","pickle.dump(NB, open('/content/Data/NB/BernoulliNB_Pipeline.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQNbuaOkBfbI"},"source":["N = ['MultinomialNB', 'ComplementNB', 'BernoulliNB']\r\n","df=pd.DataFrame(columns=(' ','','Precision', 'F1 score', 'Recall', 'Accuracy', 'Accuracy_false'))\r\n","for NB in N :\r\n","  print('Naive bayes: ',NB)\r\n","  model = pickle.load(open('/content/Data/NB/{}_Pipeline.pkl'.format(NB), 'rb'))\r\n","  Y_pred = model.predict(X_test)\r\n","  row = pd.Series({' ':NB})\r\n","  df = df.append(row, ignore_index = True )\r\n","  for a in ('macro','micro','weighted'):\r\n","    print(\"Tham số truyền vào average: \", a)\r\n","    p = precision_score(Y_test, Y_pred,average= a)\r\n","    f = f1_score(Y_test, Y_pred, average= a)\r\n","    r = recall_score(Y_test, Y_pred, average= a)\r\n","    row = pd.Series({'':a,'Precision':p, 'F1 score':f, 'Recall':r})\r\n","    df = df.append(row, ignore_index = True )\r\n","    print(\"Precision: \",p)\r\n","    print(\"F1 score: \", f)\r\n","    print(\"Recall: \", r)\r\n","  #cm = confusion_matrix(Y_test, model.predict(X_test))\r\n","  #fig, ax = plt.subplots(figsize=(14,14))\r\n","  #plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","  #plt.show()\r\n","  # Tạo một dataframe dùng để lưu lại kết quả confusion_matrix\r\n","  #da = pd.DataFrame(cm)\r\n","  # lưu lại kết quả confusion_matrix tập train\r\n","  #da.to_csv(\"/content/Data/NB/Pipeline_{}_test_cm.csv\".format(NB))\r\n","  ac = accuracy_score(Y_test,Y_pred, normalize=True)\r\n","  acf = accuracy_score(Y_test,Y_pred, normalize=False)\r\n","  row = pd.Series({'Accuracy':ac, 'Accuracy_false':acf})\r\n","  df = df.append(row, ignore_index = True )\r\n","  print(\"Accuracy: \", ac)\r\n","  print(\"Accuracy_false: \", acf)\r\n","df.to_csv('/content/Data/NB/NB_Kq_Pipeline.csv', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZoJ-xCYjg9p"},"source":["#Random Forest Classification"]},{"cell_type":"code","metadata":{"id":"E84Iy8QC_qu5"},"source":["import pickle\r\n","import pandas\r\n","import matplotlib.pyplot as plt\r\n","from sklearn.metrics import plot_confusion_matrix\r\n","from sklearn.ensemble import RandomForestClassifier\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ShSm0uJd_sGd"},"source":["X_train = pickle.load(open('/content/Data/X_train.pkl', 'rb'))\r\n","X_test = pickle.load(open('/content/Data/X_test.pkl', 'rb'))\r\n","Y_test = pickle.load(open('/content/Data/Y_test.pkl', 'rb'))\r\n","Y_train = pickle.load(open('/content/Data/Y_train.pkl', 'rb'))\r\n","X_train_trans = pickle.load(open('/content/Data/X_train_tf.pkl', 'rb'))\r\n","X_test_trans = pickle.load(open('/content/Data/X_test_tf.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxzQ_NCoj7EZ"},"source":["a = [\"gini\", \"entropy\"]\r\n","av = ['micro', 'weighted', 'macro']\r\n","for i in range(1,100):\r\n","  for j in a:\r\n","    for k in av:\r\n","      model = RandomForestClassifier(n_estimators=i, criterion = j)\r\n","      model.fit(X_train_trans, Y_train)\r\n","      score = f1_score(Y_test, model.predict(X_test_trans), average = \"micro\")\r\n","      print(i, j , score)\r\n","#title = \"CM\"\r\n","#fig, ax = plt.subplots(figsize=(14,14))\r\n","#disp = plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","#disp.ax_.set_title(title)\r\n","#print(title)\r\n","#print(disp.confusion_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cRmhHiAw_a3p"},"source":["a = [\"gini\", \"entropy\"]\r\n","for i in range(1,100):\r\n","  for j in a:\r\n","    model = RandomForestClassifier(n_estimators=i, criterion = j)\r\n","    model.fit(X_train_trans, Y_train)\r\n","    score = precision_score(Y_test, model.predict(X_test_trans), average = \"micro\")\r\n","    print(i, j , score)\r\n","#title = \"CM\"\r\n","#fig, ax = plt.subplots(figsize=(14,14))\r\n","#disp = plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","#disp.ax_.set_title(title)\r\n","#print(title)\r\n","#print(disp.confusion_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UVgZ9cIe_fUc"},"source":["a = [\"gini\", \"entropy\"]\r\n","for i in range(1,100):\r\n","  for j in a:\r\n","    model = RandomForestClassifier(n_estimators=i, criterion = j)\r\n","    model.fit(X_train_trans, Y_train)\r\n","    score = recall_score(Y_test, model.predict(X_test_trans), average = \"micro\")\r\n","    print(i, j , score)\r\n","#title = \"CM\"\r\n","#fig, ax = plt.subplots(figsize=(14,14))\r\n","#disp = plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","#disp.ax_.set_title(title)\r\n","#print(title)\r\n","#print(disp.confusion_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zmhwNa8_g0w"},"source":["a = [\"gini\", \"entropy\"]\r\n","for i in range(1,100):\r\n","  for j in a:\r\n","    model = RandomForestClassifier(n_estimators=i, criterion = j)\r\n","    model.fit(X_train_trans, Y_train)\r\n","    score = accuracy_score(Y_test, model.predict(X_test_trans), normalize=\"True\")\r\n","    print(i, j , score)\r\n","#title = \"CM\"\r\n","#fig, ax = plt.subplots(figsize=(14,14))\r\n","#disp = plot_confusion_matrix(model, X_test, Y_test,cmap=plt.cm.Blues,ax=ax)\r\n","#disp.ax_.set_title(title)\r\n","#print(title)\r\n","#print(disp.confusion_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MaEraP-SkD4o"},"source":["#LSTM"]},{"cell_type":"code","metadata":{"id":"ihmDzJyICV8K"},"source":["import pickle\r\n","import pandas as pd\r\n","import matplotlib.pyplot as plt\r\n","from sklearn.metrics import confusion_matrix\r\n","from sklearn.preprocessing import LabelEncoder\r\n","from sklearn.decomposition import TruncatedSVD\r\n","from sklearn.metrics import plot_confusion_matrix\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.feature_extraction.text import TfidfVectorizer\r\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixN8Ef9qkDFm"},"source":["# Đọc dữ liệu\r\n","X_train = pickle.load(open('Data/X_train.pkl', 'rb'))\r\n","X_test = pickle.load(open('Data/X_test.pkl', 'rb'))\r\n","Y_test = pickle.load(open('Data/Y_test.pkl', 'rb'))\r\n","Y_train = pickle.load(open('Data/Y_train.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1alNB3quCalq"},"source":["tfidf_vect = TfidfVectorizer(analyzer='word', max_features=30000)\r\n","tfidf_vect.fit(X_train)\r\n","X_data_tfidf =  tfidf_vect.transform(X_train)\r\n","X_test_tfidf =  tfidf_vect.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0qLNUNlECiTJ"},"source":["svd = TruncatedSVD(n_components=1000, random_state=42)\r\n","svd.fit(X_data_tfidf)\r\n","\r\n","X_data_tfidf_svd = svd.transform(X_data_tfidf)\r\n","X_test_tfidf_svd = svd.transform(X_test_tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_xiIGLSLCpYa"},"source":["encoder = LabelEncoder()\r\n","y_data_n = encoder.fit_transform(Y_train)\r\n","y_test_n = encoder.fit_transform(Y_test)\r\n","\r\n","encoder.classes_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YkTbAx9fCu2k"},"source":["def train_model(classifier, X_data, y_data, X_test, y_test, is_neuralnet=False, n_epochs=3):       \r\n","    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\r\n","    \r\n","    if is_neuralnet:\r\n","        classifier.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=512)\r\n","        \r\n","        val_predictions = classifier.predict(X_val)\r\n","        test_predictions = classifier.predict(X_test)\r\n","        val_predictions = val_predictions.argmax(axis=-1)\r\n","        test_predictions = test_predictions.argmax(axis=-1)\r\n","\r\n","    print(\"Validation accuracy: \", accuracy_score(val_predictions, y_val))\r\n","    print(\"Test accuracy: \", accuracy_score(test_predictions, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUoe7KxhDg6W"},"source":["def create_lstm_model():\r\n","    input_layer = Input(shape=(1000,))\r\n","    layer = Reshape((1, 1000))(input_layer)\r\n","    layer = LSTM(512, activation='relu')(layer)\r\n","    layer = Dense(512, activation='relu')(layer)\r\n","    layer = Dense(512, activation='relu')(layer)\r\n","    output_layer = Dense(27, activation='softmax')(layer)\r\n","    \r\n","    classifier = Model(input_layer, output_layer)\r\n","    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n","    \r\n","    return classifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"elqQGrpkDja0"},"source":["classifier = create_lstm_model()\r\n","train_model(classifier=classifier, X_data=X_data_tfidf_svd, y_data=y_data_n, X_test=X_test_tfidf_svd, y_test=y_test_n, is_neuralnet=True, n_epochs= 10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jl7yXwMvPyLF"},"source":[""],"execution_count":null,"outputs":[]}]}